# Neural Network From Scratch (ANN Basics)

This repository contains my practice implementation of fundamental Artificial Neural Network (ANN) concepts using Python.  
The goal of this project was to understand *how a neural network actually learns*, rather than just using libraries like TensorFlow or PyTorch.

Instead of training on large datasets, I implemented and visualized learning using logical gate classification (AND gate) â€” a classical starting point in neural network research.

---

## ğŸ“Œ Concepts Covered
- Perceptron Logic
- Binary Classification
- Decision Boundary
- Linear Separability
- Activation behavior
- Accuracy calculation
- Visualization using Matplotlib

---

## ğŸ§  What I Learned
This project helped me understand:

â€¢ How a neuron makes decisions  
â€¢ How input features affect classification  
â€¢ Why activation functions matter  
â€¢ What a decision boundary actually represents  
â€¢ Why early neural networks failed on non-linear problems (like XOR)

---

## ğŸ“Š Problem Statement
We simulate a neuron that classifies input pairs (x1, x2) into binary output using an AND logical gate.

| x1 | x2 | Output |
|----|----|------|
| 0 | 0 | 0 |
| 0 | 1 | 0 |
| 1 | 0 | 0 |
| 1 | 1 | 1 |

The model attempts to learn a separating line (decision boundary) that correctly classifies the points.

---

## ğŸ“ˆ Visualization
The decision boundary is plotted using Matplotlib to observe how the classifier separates classes in 2D feature space.

---

## ğŸ›  Tech Stack
- Python
- Numpy
- Matplotlib
- Jupyter Notebook

---

## ğŸš€ How to Run

1. Clone the repository
   git clone https://github.com/alaukikpachauri/neural-network-from-scratch.git
2. Install dependencies
   pip install -r requirements.txt
3. Run the notebook
   jupyter notebook Ann_assignment.ipynb


---

## ğŸ“ Repository Structure

neural-network-from-scratch/

â”œâ”€â”€ notebooks/
   â””â”€â”€ ann_perceptron_and_gate.ipynb


â”œâ”€â”€ docs/
   â””â”€â”€ PROBLEM_STATEMENT.md

â”œâ”€â”€ README.md

â”œâ”€â”€ requirements.txt

â””â”€â”€ .gitignore


---

## ğŸ”® Future Work
- Implement XOR classification
- Add sigmoid activation
- Multi-layer perceptron
- Backpropagation
- Gradient descent visualization

---

## ğŸ‘¨â€ğŸ’» Author
**Alaukik Pachauri**

Aspiring Machine Learning Engineer | Learning Neural Networks, Transformers & AutoML



